{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import inflect\n",
        "import unicodedata\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import joblib  # For loading the saved model (scikit-learn, etc.)\n",
        "# import your specific model loading library if it's different (e.g., TensorFlow or PyTorch)\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Dictionary mapping currency symbols to words\n",
        "currency_map = {\n",
        "        '\\u0024': 'dollar',    # $\n",
        "        '\\u00A3': 'pound',     # £\n",
        "        '\\u00A5': 'yen',       # ¥\n",
        "        '\\u20AC': 'euro',      # €\n",
        "        '\\u20B9': 'rupee',     # ₹\n",
        "        '\\u20A1': 'colon',     # ₡\n",
        "        '\\u20A2': 'cruzeiro',  # ₢\n",
        "        '\\u20A3': 'french_franc', # ₣\n",
        "        '\\u20A4': 'lira',      # ₤\n",
        "        '\\u20A5': 'mill',      # ₥\n",
        "        '\\u20A6': 'naira',     # ₦\n",
        "        '\\u20A7': 'peseta',    # ₧\n",
        "        '\\u20A8': 'rupee',     # ₨\n",
        "        '\\u20A9': 'won',       # ₩\n",
        "        '\\u20AA': 'shekel',    # ₪\n",
        "        '\\u20AB': 'dong',      # ₫\n",
        "        '\\u20AC': 'euro',      # €\n",
        "        '\\u20AD': 'kip',       # ₭\n",
        "        '\\u20AE': 'tugrik',    # ₮\n",
        "        '\\u20AF': 'drachma',   # ₯\n",
        "        '\\u20B0': 'german_penny', # ₰\n",
        "        '\\u20B1': 'peso',      # ₱\n",
        "        '\\u20B2': 'guarani',   # ₲\n",
        "        '\\u20B3': 'austral',   # ₳\n",
        "        '\\u20B4': 'hryvnia',   # ₴\n",
        "        '\\u20B5': 'cedi',      # ₵\n",
        "    }\n",
        "\n",
        "# Stop words and punctuation for processing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def convert_numbers_to_words(text):\n",
        "    \"\"\"Convert all numbers in the text to words.\"\"\"\n",
        "    return re.sub(r'\\b\\d+\\b', lambda x: p.number_to_words(x.group()), text)\n",
        "\n",
        "def clean_article(article):\n",
        "    \"\"\"\n",
        "    Cleans the given article by:\n",
        "    - Replacing currency symbols with their corresponding words\n",
        "    - Converting numbers to words\n",
        "    - Removing URLs, special characters, and extra spaces\n",
        "    - Tokenizing, removing stopwords and punctuation, and lemmatizing\n",
        "    \"\"\"\n",
        "    # Replace currency symbols with their corresponding words\n",
        "    for symbol, word in currency_map.items():\n",
        "        article = article.replace(symbol, word)\n",
        "\n",
        "    # Convert numbers to words\n",
        "    try:\n",
        "        article = convert_numbers_to_words(article)\n",
        "    except inflect.NumOutOfRangeError:\n",
        "        article = re.sub(r'\\d+', ' large_number ', article)\n",
        "\n",
        "    # Remove URLs\n",
        "    article = re.sub(r'https?://[^\\s]+', 'url', article)\n",
        "\n",
        "    # Remove special characters (except standard punctuation)\n",
        "    article = re.sub(r'[^\\w\\s.,;\\'\"-]', '', article)\n",
        "\n",
        "    # Remove extra spaces, tabs, newlines, and carriage returns\n",
        "    article = re.sub(r'\\s+', ' ', article).strip()\n",
        "\n",
        "    # Remove accented characters\n",
        "    article = unicodedata.normalize('NFKD', article).encode('ASCII', 'ignore').decode('utf-8')\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(article)\n",
        "\n",
        "    # Remove stopwords and punctuation, then lemmatize\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(token.lower())\n",
        "        for token in tokens\n",
        "        if token.lower() not in stop_words and token not in punctuation and len(token) > 1\n",
        "    ]\n",
        "\n",
        "    # Rejoin tokens into a cleaned article\n",
        "    cleaned_article = ' '.join(cleaned_tokens)\n",
        "\n",
        "    return cleaned_article\n",
        "\n",
        "# Function to predict sentiment from the article\n",
        "def predict_sentiment(article, model):\n",
        "    \"\"\"\n",
        "    Cleans the article and predicts the sentiment using the provided model.\n",
        "\n",
        "    :param article: The raw article text.\n",
        "    :param model: The trained sentiment analysis model.\n",
        "    :return: The predicted sentiment (label or score).\n",
        "    \"\"\"\n",
        "    # Clean the article\n",
        "    cleaned_article = clean_article(article)\n",
        "\n",
        "    # Assuming the model expects a list of text data\n",
        "    prediction = model.predict([cleaned_article])\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example article\n",
        "    article = \"\"\"\n",
        "    Barcelona have reportedly made a $100 million offer for the star player.\n",
        "    Sources suggest a €50 million deal might also be on the table. More details: https://sportsnews.com/article\n",
        "    \"\"\"\n",
        "\n",
        "    # Load your pre-trained model (e.g., joblib for scikit-learn models)\n",
        "    # Replace 'path_to_model.pkl' with the path to your saved model\n",
        "    model = joblib.load('path_to_model.pkl')  # or load model in another format if different\n",
        "\n",
        "    # Predict sentiment\n",
        "    sentiment_prediction = predict_sentiment(article, model)\n",
        "\n",
        "    print(\"Predicted Sentiment:\", sentiment_prediction)"
      ],
      "metadata": {
        "id": "pm8wbfqamFbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gr2HTaTSmPUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}